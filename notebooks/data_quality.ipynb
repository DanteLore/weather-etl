{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min and Max Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from helpers.aws import execute_athena_query\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sql = f\"\"\"\n",
    "select\n",
    "  min(temperature) as min_temp,\n",
    "  max(temperature) as max_temp,\n",
    "  avg(temperature) as mean_temp,\n",
    "  month\n",
    "from (\n",
    "        select\n",
    "            temperature,\n",
    "            date_trunc('month', observation_ts) as month\n",
    "        from incoming.weather\n",
    "        where year <> '2022' and month <> '7' and site_name <> 'CHIVENOR' or temperature > -5 -- exclude broken readings from Chivenor\n",
    "    )\n",
    "group by month\n",
    "order by month\n",
    "\"\"\"\n",
    "\n",
    "results_url = execute_athena_query(sql, \"lake\", \"dantelore.queryresults\")\n",
    "df = pd.read_csv(results_url)\n",
    "df['month'] = pd.to_datetime(df['month']).dt.strftime('%B %Y')\n",
    "df[\"mean_temp\"] = df[\"mean_temp\"].round(1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.set_title('UK Temperature Summary', fontdict={'fontsize': '18', 'fontweight' : '3'})\n",
    "\n",
    "# Ensure numeric types for plotting\n",
    "df['min_temp'] = pd.to_numeric(df['min_temp'], errors='coerce')\n",
    "df['max_temp'] = pd.to_numeric(df['max_temp'], errors='coerce')\n",
    "df['mean_temp'] = pd.to_numeric(df['mean_temp'], errors='coerce')\n",
    "\n",
    "ax.plot(df.index, df['mean_temp'], linewidth=4.0, label=\"Mean Temperature\")\n",
    "ax.fill_between(df.index, df['min_temp'], df['max_temp'], alpha=0.5, label=\"Temperature Range\")\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Temperature', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(df.index, df[\"month\"].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "select site_id, site_name, observation_ts, temperature, lat, lon \n",
    "from weather \n",
    "where year = 2022 and month = 7 \n",
    "order by temperature asc \n",
    "limit 3\n",
    "\"\"\"\n",
    "\n",
    "results_url = execute_athena_query(sql, \"lake\", \"dantelore.queryresults\")\n",
    "cold_places = pd.read_csv(results_url)\n",
    "cold_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "select observation_ts, temperature \n",
    "from weather \n",
    "where year = 2022 and month = 7\n",
    "  and observation_ts between TIMESTAMP '2022-07-03 00:00:00' and TIMESTAMP '2022-07-10 00:00:00' \n",
    "  and site_id = '3707' \n",
    "order by observation_ts asc\n",
    "\"\"\"\n",
    "\n",
    "results_url = execute_athena_query(sql, \"lake\", \"dantelore.queryresults\")\n",
    "chivenor_july = pd.read_csv(results_url)\n",
    "\n",
    "\n",
    "chivenor_july['xlabel'] = pd.to_datetime(chivenor_july['observation_ts']).dt.strftime('%d %B %H:%m')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.set_title('RMB Chivenor, July Temperatures', fontdict={'fontsize': '18', 'fontweight' : '3'})\n",
    "\n",
    "ax.plot(chivenor_july.index, chivenor_july.temperature, linewidth=4.0, label=\"Temperature\")\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Temperature', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(chivenor_july.index, chivenor_july[\"xlabel\"].values)\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Duplciates\n",
    "\n",
    "## Duplicates by Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "select\n",
    "  count(*) as num_rows,\n",
    "  cast(year as int) as year,\n",
    "  cast(month as int) as month\n",
    "from weather\n",
    "group by year, month\n",
    "order by num_rows desc\n",
    "\"\"\"\n",
    "\n",
    "results_url = execute_athena_query(sql, \"lake\", \"dantelore.queryresults\")\n",
    "df1 = pd.read_csv(results_url)\n",
    "\n",
    "df1.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates by Event Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql = f\"\"\"\n",
    "select\n",
    "    obs_period,\n",
    "    count(*) as num_rows from\n",
    "    (\n",
    "        select\n",
    "          *,\n",
    "          date_trunc('day', observation_ts) as obs_period\n",
    "          from weather\n",
    "    )\n",
    "group by obs_period\n",
    "order by num_rows desc\n",
    "\"\"\"\n",
    "\n",
    "results_url = execute_athena_query(sql, \"lake\", \"dantelore.queryresults\")\n",
    "df2 = pd.read_csv(results_url)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of row counts\n",
    "\n",
    "Clearly most days have the 'right' number of rows, but there are a few outliers with too many or too few.\n",
    "\n",
    "See below cumulative distribution (ECFD) and Kernel density estimation (KDE) for the number of rows vs days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "figure.suptitle('Distribution of row counts')\n",
    "axes[0].set_title('KDE Plot')\n",
    "axes[1].set_title('ECFD Plot')\n",
    "\n",
    "sns.kdeplot(df2, ax=axes[0], x=\"num_rows\", cut=0, fill=True)\n",
    "sns.ecdfplot(df1, ax=axes[1], x=\"num_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "select\n",
    "    obs_period,\n",
    "    count(*) as num_rows\n",
    "from\n",
    "(\n",
    "    select\n",
    "        *,\n",
    "        date_trunc('day', observation_ts) as obs_period,\n",
    "        ROW_NUMBER() OVER ( partition by date_trunc('hour', observation_ts), site_id order by observation_ts desc ) as rn\n",
    "       from weather\n",
    ")\n",
    "where rn = 1\n",
    "group by obs_period\n",
    "order by num_rows desc\n",
    "\"\"\"\n",
    "\n",
    "results_url = execute_athena_query(sql, \"lake\", \"dantelore.queryresults\")\n",
    "deduped_df = pd.read_csv(results_url)\n",
    "deduped_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "figure.suptitle('Distribution of row counts after deduplication')\n",
    "axes[0].set_title('KDE Plot')\n",
    "axes[1].set_title('ECFD Plot')\n",
    "\n",
    "sns.kdeplot(deduped_df, ax=axes[0], x=\"num_rows\", cut=0, fill=True)\n",
    "sns.kdeplot(df1, ax=axes[0], x=\"num_rows\", cut=0, fill=True)\n",
    "sns.ecdfplot(deduped_df, ax=axes[1], x=\"num_rows\")\n",
    "sns.ecdfplot(df1, ax=axes[1], x=\"num_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days with less than 90% of expected rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest = deduped_df.num_rows.max() # Somewhat lazy way to get the max possible number of measurements in a day - doesn't account for sites added and removed over time etc\n",
    "low_cutoff = round(highest * 0.90)\n",
    "\n",
    "low_days = deduped_df[deduped_df.num_rows < low_cutoff].sort_values(by=['num_rows'])\n",
    "low_days[\"percentage\"] = round(low_days[\"num_rows\"] * 100 / highest)\n",
    "\n",
    "print(f\"Max value: {highest} Cutoff @ 90%: {low_cutoff} Number of days below cutoff: {len(low_days)} which is {round(len(low_days) * 100 / len(df2), 2)}% of the total\")\n",
    "\n",
    "low_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days with duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cutoff = highest\n",
    "high_days = df2[df2.num_rows > high_cutoff].sort_values(by=['num_rows'], ascending=False)\n",
    "print(f\"Number of days above max: {len(high_days)} which is {round(len(high_days) * 100 / len(df2), 2)}% of the total\")\n",
    "\n",
    "high_days[\"percentage\"] = round(high_days[\"num_rows\"] * 100 / highest)\n",
    "\n",
    "high_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
